raw<-read.csv("ks-projects-201801.csv", header= TRUE)
clean1 <- raw[raw$state != "live",]
clean1 <- clean1[clean1$state != "canceled",]
yourData <- clean1[clean1$state != "undefined",]
#empty vectors to store everything
precisionsVec<-vector() #for storing performance measures for all 10 models
recallsVec<-vector()
accuracyVec<-vector()
fmeasureVec<-vector()
#Randomly shuffle the data
set.seed(88) #set seed so we can replicate our reults every time
shuffledData<-yourData[sample(nrow(yourData)),]
# Divide dataset by 10 for ten folds, start rowCount counter at 1
partitionSize <- round(nrow(yourData)/10) - 1
print(partitionSize)
rowCount <- 1
#Perform 10 fold cross validation
for(i in 1:10){
# Upperbound is basically rowCount + fold/partition size
upperBound <- min(rowCount + partitionSize, nrow(shuffledData))
testData <- shuffledData[rowCount:upperBound,]
trainData <- shuffledData[-(rowCount:upperBound),]
#Use the test and train data partitions however you desire...
classifier <- naiveBayes(trainData[,2:10], trainData[,11])
#confusion matrix
tab<-table(predict(classifier, testData[,-11]), testData[,11])
#calculating performance measures
accuracy <- sum(diag(tab))/sum(tab)
precision <- tab[1,1] / sum(tab[1,]) # Precision = TP / (TP + FP)
recall <- tab[1,1] / sum(tab[,1]) # Recall = TP / (TP + FN)
fmeasure <- 2 * precision * recall / (precision + recall)
#adding performance measures in
accuracyVec<-c(accuracyVec,accuracy)
precisionsVec<-c(precisionsVec, precision)
recallsVec<-c(recallsVec,recall)
fmeasureVec<-c(fmeasureVec, fmeasure )
#Increment rowCount for next iteration
rowCount <- rowCount + partitionSize + 1
}
library(e1071)
#empty vectors to store everything
precisionsVec<-vector() #for storing performance measures for all 10 models
recallsVec<-vector()
accuracyVec<-vector()
fmeasureVec<-vector()
#Randomly shuffle the data
set.seed(88) #set seed so we can replicate our reults every time
shuffledData<-yourData[sample(nrow(yourData)),]
# Divide dataset by 10 for ten folds, start rowCount counter at 1
partitionSize <- round(nrow(yourData)/10) - 1
print(partitionSize)
rowCount <- 1
#Perform 10 fold cross validation
for(i in 1:10){
# Upperbound is basically rowCount + fold/partition size
upperBound <- min(rowCount + partitionSize, nrow(shuffledData))
testData <- shuffledData[rowCount:upperBound,]
trainData <- shuffledData[-(rowCount:upperBound),]
#Use the test and train data partitions however you desire...
classifier <- naiveBayes(trainData[,2:10], trainData[,11])
#confusion matrix
tab<-table(predict(classifier, testData[,-11]), testData[,11])
#calculating performance measures
accuracy <- sum(diag(tab))/sum(tab)
precision <- tab[1,1] / sum(tab[1,]) # Precision = TP / (TP + FP)
recall <- tab[1,1] / sum(tab[,1]) # Recall = TP / (TP + FN)
fmeasure <- 2 * precision * recall / (precision + recall)
#adding performance measures in
accuracyVec<-c(accuracyVec,accuracy)
precisionsVec<-c(precisionsVec, precision)
recallsVec<-c(recallsVec,recall)
fmeasureVec<-c(fmeasureVec, fmeasure )
#Increment rowCount for next iteration
rowCount <- rowCount + partitionSize + 1
}
memory.limit()
memory.limit(size = 5000)
library(e1071)
#empty vectors to store everything
precisionsVec<-vector() #for storing performance measures for all 10 models
recallsVec<-vector()
accuracyVec<-vector()
fmeasureVec<-vector()
#Randomly shuffle the data
set.seed(88) #set seed so we can replicate our reults every time
shuffledData<-yourData[sample(nrow(yourData)),]
# Divide dataset by 10 for ten folds, start rowCount counter at 1
partitionSize <- round(nrow(yourData)/10) - 1
print(partitionSize)
rowCount <- 1
#Perform 10 fold cross validation
for(i in 1:10){
# Upperbound is basically rowCount + fold/partition size
upperBound <- min(rowCount + partitionSize, nrow(shuffledData))
testData <- shuffledData[rowCount:upperBound,]
trainData <- shuffledData[-(rowCount:upperBound),]
#Use the test and train data partitions however you desire...
classifier <- naiveBayes(trainData[,2:10], trainData[,11])
#confusion matrix
tab<-table(predict(classifier, testData[,-11]), testData[,11])
#calculating performance measures
accuracy <- sum(diag(tab))/sum(tab)
precision <- tab[1,1] / sum(tab[1,]) # Precision = TP / (TP + FP)
recall <- tab[1,1] / sum(tab[,1]) # Recall = TP / (TP + FN)
fmeasure <- 2 * precision * recall / (precision + recall)
#adding performance measures in
accuracyVec<-c(accuracyVec,accuracy)
precisionsVec<-c(precisionsVec, precision)
recallsVec<-c(recallsVec,recall)
fmeasureVec<-c(fmeasureVec, fmeasure )
#Increment rowCount for next iteration
rowCount <- rowCount + partitionSize + 1
}
raw<-read.csv("ks-projects-201801.csv", header= TRUE)
clean1 <- raw[raw$state != "live",]
clean1 <- clean1[clean1$state != "canceled",]
yourData <- clean1[clean1$state != "undefined",]
library(e1071)
#empty vectors to store everything
precisionsVec<-vector() #for storing performance measures for all 10 models
recallsVec<-vector()
accuracyVec<-vector()
fmeasureVec<-vector()
#Randomly shuffle the data
set.seed(88) #set seed so we can replicate our reults every time
shuffledData<-yourData[sample(nrow(yourData)),]
# Divide dataset by 10 for ten folds, start rowCount counter at 1
partitionSize <- round(nrow(yourData)/10) - 1
print(partitionSize)
rowCount <- 1
#Perform 10 fold cross validation
for(i in 1:10){
# Upperbound is basically rowCount + fold/partition size
upperBound <- min(rowCount + partitionSize, nrow(shuffledData))
testData <- shuffledData[rowCount:upperBound,]
trainData <- shuffledData[-(rowCount:upperBound),]
#Use the test and train data partitions however you desire...
classifier <- naiveBayes(trainData[,2:10], trainData[,11])
#confusion matrix
tab<-table(predict(classifier, testData[,-11]), testData[,11])
#calculating performance measures
accuracy <- sum(diag(tab))/sum(tab)
precision <- tab[1,1] / sum(tab[1,]) # Precision = TP / (TP + FP)
recall <- tab[1,1] / sum(tab[,1]) # Recall = TP / (TP + FN)
fmeasure <- 2 * precision * recall / (precision + recall)
#adding performance measures in
accuracyVec<-c(accuracyVec,accuracy)
precisionsVec<-c(precisionsVec, precision)
recallsVec<-c(recallsVec,recall)
fmeasureVec<-c(fmeasureVec, fmeasure )
#Increment rowCount for next iteration
rowCount <- rowCount + partitionSize + 1
}
library(e1071)
#empty vectors to store everything
precisionsVec<-vector() #for storing performance measures for all 10 models
recallsVec<-vector()
accuracyVec<-vector()
fmeasureVec<-vector()
#Randomly shuffle the data
set.seed(88) #set seed so we can replicate our reults every time
shuffledData<-yourData[sample(nrow(yourData)),]
# Divide dataset by 10 for ten folds, start rowCount counter at 1
partitionSize <- round(nrow(yourData)/5) - 1
print(partitionSize)
rowCount <- 1
#Perform 10 fold cross validation
for(i in 1:5){
# Upperbound is basically rowCount + fold/partition size
upperBound <- min(rowCount + partitionSize, nrow(shuffledData))
testData <- shuffledData[rowCount:upperBound,]
trainData <- shuffledData[-(rowCount:upperBound),]
#Use the test and train data partitions however you desire...
classifier <- naiveBayes(trainData[,2:10], trainData[,11])
#confusion matrix
tab<-table(predict(classifier, testData[,-11]), testData[,11])
#calculating performance measures
accuracy <- sum(diag(tab))/sum(tab)
precision <- tab[1,1] / sum(tab[1,]) # Precision = TP / (TP + FP)
recall <- tab[1,1] / sum(tab[,1]) # Recall = TP / (TP + FN)
fmeasure <- 2 * precision * recall / (precision + recall)
#adding performance measures in
accuracyVec<-c(accuracyVec,accuracy)
precisionsVec<-c(precisionsVec, precision)
recallsVec<-c(recallsVec,recall)
fmeasureVec<-c(fmeasureVec, fmeasure )
#Increment rowCount for next iteration
rowCount <- rowCount + partitionSize + 1
}
library(e1071)
#empty vectors to store everything
precisionsVec<-vector() #for storing performance measures for all 10 models
recallsVec<-vector()
accuracyVec<-vector()
fmeasureVec<-vector()
#Randomly shuffle the data
set.seed(88) #set seed so we can replicate our reults every time
shuffledData<-yourData[sample(nrow(yourData)),]
# Divide dataset by 10 for ten folds, start rowCount counter at 1
partitionSize <- round(nrow(yourData)/10) - 1
print(partitionSize)
rowCount <- 1
#Perform 10 fold cross validation
for(i in 1:10){
# Upperbound is basically rowCount + fold/partition size
upperBound <- min(rowCount + partitionSize, nrow(shuffledData))
testData <- shuffledData[rowCount:upperBound,]
trainData <- shuffledData[-(rowCount:upperBound),]
#Use the test and train data partitions however you desire...
classifier <- naiveBayes(trainData[,2:10], trainData[,11])
#confusion matrix
tab<-table(predict(classifier, testData[,-11]), testData[,11])
#calculating performance measures
accuracy <- sum(diag(tab))/sum(tab)
precision <- tab[1,1] / sum(tab[1,]) # Precision = TP / (TP + FP)
recall <- tab[1,1] / sum(tab[,1]) # Recall = TP / (TP + FN)
fmeasure <- 2 * precision * recall / (precision + recall)
#adding performance measures in
accuracyVec<-c(accuracyVec,accuracy)
precisionsVec<-c(precisionsVec, precision)
recallsVec<-c(recallsVec,recall)
fmeasureVec<-c(fmeasureVec, fmeasure )
#Increment rowCount for next iteration
rowCount <- rowCount + partitionSize + 1
}
