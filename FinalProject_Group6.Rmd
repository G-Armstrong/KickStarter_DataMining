---
start date: 4/21/2021
authors: Grant Armstrong, Aatika Rizwan, Damon Luk, Rishab Ayyappath, Jeremy Amand
title: "KickStarter DataMining"
output: html_notebook
---
Columns of interest include launched, state, main_category, category, deadline, backers, and country
 
Aatika - Import Data & Clean (remove all state='live' and 'canceled' rows) Make sure CSV is readable from the internet so we dont need
to keep it locally in our repository 
```{r}
raw<-read.csv("https://www.kaggle.com/kemical/kickstarter-projects?select=ks-projects-201801.csv")
```

supervised classification method 1 -predicting the value of the ‘state’ column (whether or not the project succeeds) based on values of other attributes.
Grant & Damon - Naive Bayes w/ 10 fold cross validation
```{r}
#empty vectors to store everything
precisionsVec<-vector() #for storing performance measures for all 10 models
recallsVec<-vector()
accuracyVec<-vector()
fmeasureVec<-vector()

#Randomly shuffle the data
set.seed(88) #set seed so we can replicate our reults every time
shuffledData<-yourData[sample(nrow(yourData)),]

# Divide dataset by 10 for ten folds, start rowCount counter at 1
partitionSize <- round(nrow(yourData)/10) - 1
print(partitionSize) 
rowCount <- 1

#Perform 10 fold cross validation
for(i in 1:10){
    # Upperbound is basically rowCount + fold/partition size
    upperBound <- min(rowCount + partitionSize, nrow(shuffledData))
    testData <- shuffledData[rowCount:upperBound,]
    trainData <- shuffledData[-(rowCount:upperBound),]

    #Use the test and train data partitions however you desire...
    classifier <- naiveBayes(trainData[,2:10], trainData[,11])
    #confusion matrix 
    tab<-table(predict(classifier, testData[,-11]), testData[,11])
    
    #calculating performance measures
    accuracy <- sum(diag(tab))/sum(tab)
    precision <- tab[1,1] / sum(tab[1,]) # Precision = TP / (TP + FP)
    recall <- tab[1,1] / sum(tab[,1]) # Recall = TP / (TP + FN)
    fmeasure <- 2 * precision * recall / (precision + recall)
    
    #adding performance measures in 
    accuracyVec<-c(accuracyVec,accuracy)
    precisionsVec<-c(precisionsVec, precision)
    recallsVec<-c(recallsVec,recall)
    fmeasureVec<-c(fmeasureVec, fmeasure )
    
    #Increment rowCount for next iteration
    rowCount <- rowCount + partitionSize + 1
}

```

Performance of Naive Bayes
```{r}
print("Average Precision:")
mean(precisionsVec)
print("Average Recall:" )
mean(recallsvec)
print("Average Accuracy:" )
mean(accuracyVec)
print("Average F-measure:")
mean(fmeasureVec)
```


We are thinking of using a decision tree as another classification method 
to visualize the values of attributes that lead to success or failure
Jeremy & Rishab
```{r}

```

Performance of DT
```{r}


```


ROCR comparing Naive-Bayes to DT
```{r}

```


Aatika
Word Cloud of Successful Project Names
```{r}

```

