---
start date: 4/21/2021
authors: Grant Armstrong, Aatika Rizwan, Damon Luk, Rishab Ayyappath, Jeremy Amand
title: "KickStarter DataMining"
output: html_notebook
---
Columns of interest include launched, state, main_category, category, deadline, backers, and country
 
Aatika - Import Data & Clean (remove all state='live' and 'canceled' rows) 
```{r}
raw<-read.csv("ks-projects-201801.csv", header= TRUE, stringsAsFactors = TRUE) # read in local file
#clean1 <- raw[raw$state %in% c("failed", "successful"),] #only want data that is confimed fail or success

clean1 <- raw[raw$state != "live",]
clean1 <- clean1[clean1$state != "canceled",]
clean1 <- clean1[clean1$state != "suspended",]
clean1$ID <- NULL # not needed for prediction
clean1 <- clean1[clean1$state != "undefined",]
clean1$state <- droplevels(clean1$state, exclude = "live")

clean1$ID <- NULL # ID are not needed to predict on 

clean1$launchToDate <- abs(as.Date(clean1[,"launched"])-as.Date(clean1[,"deadline"])) #days between launch and deadline
clean1$deadline <- NULL
clean1$launched <- NULL

clean1$amountPerBacker <- clean1[,"pledged"]/(clean1[,"backers"]) #average amount spent by each backer
clean1 <- clean1[!is.nan(clean1$amountPerBacker) ,] #remove rows that do not have defined number for avg amnt per backer

clean1$pledged <- NULL #not needed for predicting
clean1$backers <- NULL #same
clean1$difmoney <- clean1$goal - clean1$usd.pledged # the lower the better? 

clean1$usd_goal_real <- NULL  #omitted
clean1$usd_pledged_real <- NULL #omitted
#clean1$goal <- NULL #not needed after above calc
clean1$usd.pledged <-NULL  #same
clean1 <- na.omit(clean1)
yourData <- clean1 # to preserve name column
yourData$name <- NULL #not needed for prediction
```

supervised classification method 1 -predicting the value of the ‘state’ column (whether or not the project succeeds) based on values of other attributes.
Grant & Damon - Naive Bayes w/ 10 fold cross validation
```{r}
library(e1071)

#empty vectors to store everything
precisionsVec<-vector() #for storing performance measures for all 10 models
recallsVec<-vector()
accuracyVec<-vector()
fmeasureVec<-vector()

#Randomly shuffle the data
set.seed(88) #set seed so we can replicate our results every time
shuffledData<-yourData[sample(nrow(yourData)),]

# Divide dataset by 10 for ten folds, start rowCount counter at 1
partitionSize <- round(nrow(yourData)/10) - 1
print(partitionSize) 
rowCount <- 1

#Perform 10 fold cross validation
for(i in 1:10){
    # Upperbound is basically rowCount + fold/partition size
    upperBound <- min(rowCount + partitionSize, nrow(shuffledData))
    testData <- shuffledData[rowCount:upperBound,]
    trainData <- shuffledData[-(rowCount:upperBound),]

    #Use the test and train data partitions however you desire...
    classifier <- naiveBayes(trainData[,-5],trainData[,"state"])
    #confusion matrix 
    pred = predict(classifier, testData, type = "class")
    tab <- table(pred, testData$state)
    
    #calculating performance measures
    accuracy <- sum(diag(tab))/sum(tab)
    precision <- tab["successful","successful"] / sum(tab["successful",]) # Precision = TP / (TP + FP)
    recall <- tab["successful","successful"] / sum(tab[,"successful"]) # Recall = TP / (TP + FN)
    fmeasure <- 2 * precision * recall / (precision + recall)
    
    #adding performance measures in 
    accuracyVec<-c(accuracyVec,accuracy)
    precisionsVec<-c(precisionsVec, precision)
    recallsVec<-c(recallsVec,recall)
    fmeasureVec<-c(fmeasureVec, fmeasure )
    
    #Increment rowCount for next iteration
    rowCount <- rowCount + partitionSize + 1
}

```

Performance of Naive Bayes
```{r}
print("Average Precision:")
mean(precisionsVec)
print("Average Recall:" )
mean(recallsVec)
print("Average Accuracy:" )
mean(accuracyVec)
print("Average F-measure:")
mean(fmeasureVec)
```


We are thinking of using a decision tree as another classification method 
to visualize the values of attributes that lead to success or failure
Jeremy & Rishab - Decision Tree w/ 10 cross-fold validation

```{r}
install.packages("rpart.plot") #can plot dt w/ prp(tree,type=1,extra=0)
```

```{r}
library(rpart)
library(rpart.plot)
library(e1071)
set.seed(88) #for consistent results

yourData <- yourData[sample(nrow(yourData)),] #shuffle data
#yourData$launchToDate <- abs(as.Date(yourData[,"launched"])-as.Date(yourData[,"deadline"])) #days between launch and deadline
#yourData$amountPerBacker <- yourData[,"pledged"]/(yourData[,"backers"]) #average amount spent by each backer
folds <- cut(seq(1,nrow(yourData)),breaks=10,labels=FALSE) #split data into 10 folds
#performance metrics
dtprecision=0
dtrecall=0
dtaccuracy=0
dtfmeasure=0
nbprecision=0
nbrecall=0
nbaccuracy=0
nbfmeasure=0
#10-fold cross validation
for(i in 1:10){
  index <- which(folds==i,arr.ind=TRUE)
  training <- yourData[-index,]
  testing <- yourData[index,]
  tree <- rpart(state~goal+main_category+launchToDate+amountPerBacker+country,data=training,method="class") #dt based off of 5 categories
  #get prediction and confusion matrix
  dtprediction <- predict(tree,testing,type="class")
  dtcm <- table(testing$state,dtprediction)
  #performance metrics
  dtprecision <- (dtprecision*(i-1)+(dtcm["successful","successful"]/sum(dtcm["successful",])))/i
  dtrecall <- (dtrecall*(i-1)+(dtcm["successful","successful"]/sum(dtcm[,"successful"])))/i
  dtaccuracy <- (dtaccuracy*(i-1)+(sum(diag(dtcm))/sum(dtcm)))/i
  dtfmeasure <- (dtfmeasure*(i-1)+(2*dtprecision*dtrecall)/(dtprecision+dtrecall))/i
  
  nb <- naiveBayes(training[,-5],training[,"state"])
  #nb <- naiveBayes(state~)
  nbprediction <- predict(nb,testing,type="class")
  nbcm <- table(testing$state,nbprediction)
  nbprecision <- (nbprecision*(i-1)+(nbcm["successful","successful"]/sum(nbcm["successful",])))/i
  nbrecall <- (nbrecall*(i-1)+(nbcm["successful","successful"]/sum(nbcm[,"successful"])))/i
  nbaccuracy <- (nbaccuracy*(i-1)+(sum(diag(nbcm))/sum(nbcm)))/i
  nbfmeasure <- (nbfmeasure*(i-1)+(2*nbprecision*nbrecall)/(nbprecision+nbrecall))/i
  nbcm
}
```

Performance
```{r}
print("Decision Tree Performance:")
print("Average Precision:")
dtprecision
print("Average Recall:" )
dtrecall
print("Average Accuracy:" )
dtaccuracy
print("Average F-measure:")
dtfmeasure
print("Naive Bayes Performance:")
print("Average Precision:")
nbprecision
print("Average Recall:" )
nbrecall
print("Average Accuracy:" )
nbaccuracy
print("Average F-measure:")
nbfmeasure
```

Aatika
ROCR comparing Naive-Bayes to DT
```{r}
library(caTools)
library(ROCR)

set.seed(88)  

split <- sample.split(yourData$state, SplitRatio = 0.8)
train <- subset(yourData, split == TRUE)
test <- subset(yourData, split == FALSE)

#glm.fit <- glm(class ~ ., data = train, family = binomial)
# Use the model to predict the test set
#glm.probs <- predict(glm.fit, newdata=test, type="response")

tree <- rpart(state~goal+main_category+launchToDate+amountPerBacker+country, data=train, method="class") #dt based off of 5 categories
  #get prediction and confusion matrix
dtprediction <- predict(tree, newdata=test ,type="class")

dtPredROC <- prediction(dtprediction, test$state)
dtPerfROC <- performance(dtPredROC, 'tpr', 'fpr')
plot(dtPerfROC, col=2, text.adj = c(-0.2, 1.7), main="ROC Curve")

train$goal <-NULL
test$goal <-NULL
classifier <- naiveBayes(state ~. , data=train)
pred = predict(classifier, test, type="raw")

bayesPredROC <- prediction(pred[,2], test$state)
bayesPerfROC <- performance(bayesPredROC, 'tpr', 'fpr')
plot(bayesPerfROC, col=3, add=TRUE)
```



