---
start date: 4/21/2021
authors: Grant Armstrong, Aatika Rizwan, Damon Luk, Rishab Ayyappath, Jeremy Amand
title: "KickStarter DataMining"
output: html_notebook
---
Columns of interest include launched, state, main_category, category, deadline, backers, and country
 
Aatika - Import Data & Clean (remove all state='live' and 'canceled' rows) Make sure CSV is readable from the internet so we dont need
to keep it locally in our repository 
```{r}
raw<-read.csv("ks-projects-201801.csv", header= TRUE, stringsAsFactors = TRUE)


clean1 <- raw[raw$state != "live",]
clean1 <- clean1[clean1$state != "canceled",]
clean1 <- clean1[clean1$state != "suspended",]
clean1$ID <- NULL
clean1 <- clean1[clean1$state != "undefined",]
clean1$launchToDate <- abs(as.Date(clean1[,"launched"])-as.Date(clean1[,"deadline"])) #days between launch and deadline
clean1$deadline <- NULL
clean1$launched <- NULL

clean1$amountPerBacker <- clean1[,"pledged"]/(clean1[,"backers"]) #average amount spent by each backer
clean1$pledged <- NULL
clean1$backers <- NULL
clean1$difmoney <- clean1$goal - clean1$usd.pledged # the lower the better? 

clean1$usd_goal_real <- NULL 
clean1$usd_pledged_real <- NULL
clean1$goal <- NULL
clean1$usd.pledged <-NULL 
yourData <- clean1 # to preserve name column
yourData$name <- NULL
```

supervised classification method 1 -predicting the value of the ‘state’ column (whether or not the project succeeds) based on values of other attributes.
Grant & Damon - Naive Bayes w/ 10 fold cross validation
```{r}
library(e1071)

#empty vectors to store everything
precisionsVec<-vector() #for storing performance measures for all 10 models
recallsVec<-vector()
accuracyVec<-vector()
fmeasureVec<-vector()

#Randomly shuffle the data
set.seed(88) #set seed so we can replicate our results every time
shuffledData<-yourData[sample(nrow(yourData)),]

# Divide dataset by 10 for ten folds, start rowCount counter at 1
partitionSize <- round(nrow(yourData)/10) - 1
print(partitionSize) 
rowCount <- 1

#Perform 10 fold cross validation
for(i in 1:10){
    # Upperbound is basically rowCount + fold/partition size
    upperBound <- min(rowCount + partitionSize, nrow(shuffledData))
    testData <- shuffledData[rowCount:upperBound,]
    trainData <- shuffledData[-(rowCount:upperBound),]

    #Use the test and train data partitions however you desire...
    classifier <- naiveBayes(trainData[,-6], trainData[,6])
    #confusion matrix 
    tab<-table(predict(classifier, testData[,-6]), testData[,6])
    
    #calculating performance measures
    accuracy <- sum(diag(tab))/sum(tab)
    precision <- tab[1,1] / sum(tab[1,]) # Precision = TP / (TP + FP)
    recall <- tab[1,1] / sum(tab[,1]) # Recall = TP / (TP + FN)
    fmeasure <- 2 * precision * recall / (precision + recall)
    
    #adding performance measures in 
    accuracyVec<-c(accuracyVec,accuracy)
    precisionsVec<-c(precisionsVec, precision)
    recallsVec<-c(recallsVec,recall)
    fmeasureVec<-c(fmeasureVec, fmeasure )
    
    #Increment rowCount for next iteration
    rowCount <- rowCount + partitionSize + 1
}

```

Performance of Naive Bayes
```{r}
print("Average Precision:")
mean(precisionsVec)
print("Average Recall:" )
mean(recallsVec)
print("Average Accuracy:" )
mean(accuracyVec)
print("Average F-measure:")
mean(fmeasureVec)
```


We are thinking of using a decision tree as another classification method 
to visualize the values of attributes that lead to success or failure
Jeremy & Rishab - Decision Tree w/ 10 cross-fold validation

```{r}
install.packages("rpart.plot") #can plot dt w/ prp(tree,type=1,extra=1)
```

```{r}
library(rpart)
library(rpart.plot)
set.seed(88) #for consistent results

yourData <- yourData[sample(nrow(yourData)),] #shuffle data

#yourData$launchToDate <- abs(as.Date(yourData[,"launched"])-as.Date(yourData[,"deadline"])) #days between launch and deadline
#yourData$amountPerBacker <- yourData[,"pledged"]/(yourData[,"backers"]) #average amount spent by each backer
folds <- cut(seq(1,nrow(yourData)),breaks=10,labels=FALSE) #split data into 10 folds
#performance metrics
dtprecision=0
dtrecall=0
dtaccuracy=0
dtfmeasure=0
#10-fold cross validation
for(i in 1:10){
  index <- which(folds==i,arr.ind=TRUE)
  training <- yourData[-index,]
  testing <- yourData[index,]
  tree <- rpart(state~goal+main_category+launchToDate+amountPerBacker+country,data=training,method="class") #dt based off of 5 categories
  #get prediction and confusion matrix
  dtprediction <- predict(tree,testing,type="class")
  dtcm <- table(testing$state,dtprediction)
  #performance metrics
  dtprecision <- (dtprecision*(i-1)+(dtcm[1,1]/sum(dtcm[1,])))/i
  dtrecall <- (dtrecall*(i-1)+(dtcm[1,1]/sum(dtcm[,1])))/i
  dtaccuracy <- (dtaccuracy*(i-1)+(sum(diag(dtcm))/sum(dtcm)))/i
  dtfmeasure <- (dtfmeasure*(i-1)+(2*dtprecision*dtrecall)/(dtprecision+dtrecall))/i
}
```

Performance of DT
```{r}
print("Decision Tree Performance:")
print("Average Precision:")
dtprecision
print("Average Recall:" )
dtrecall
print("Average Accuracy:" )
dtaccuracy
print("Average F-measure:")
dtfmeasure
```


ROCR comparing Naive-Bayes to DT
```{r}

```


Aatika
Word Cloud of Successful Project Names
```{r}

```

